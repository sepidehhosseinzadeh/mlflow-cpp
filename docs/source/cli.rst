.. _cli:

Command-Line Interface
======================

The MLflow command-line interface (CLI) provides a simple interface to various functionality in MLflow. You can use the CLI to
run projects, start the tracking UI, create and list experiments, download run artifacts,
serve MLflow Python Function and scikit-learn models, and serve models on
`Microsoft Azure Machine Learning <https://azure.microsoft.com/en-us/overview/machine-learning/>`_ and
`Amazon SageMaker <https://aws.amazon.com/sagemaker/>`_.

.. code::

    $ mlflow --help
    Usage: mlflow [OPTIONS] COMMAND [ARGS]...

    Options:
      --version  Show the version and exit.
      --help     Show this message and exit.

    Commands:
      azureml      Serve models on Azure Machine Learning.
      download     Download the artifact at the specified DBFS or S3 URI. 
      experiments  Manage experiments.
      pyfunc       Serve Python models locally.
      run          Run an MLflow project from the given URI.
      sagemaker    Serve models on Amazon SageMaker.
      sklearn      Serve scikit-learn models.
      ui           Run the MLflow tracking UI.


Each individual command has a detailed help screen accessible via ``mlflow command_name --help``.

.. contents:: Table of Contents
  :local:
  :depth: 2


Azure Machine Learning Models
-----------------------------

Subcommands to serve models on Azure Machine Learning.


Download
--------

Download the artifact at the specified DBFS or S3 URI into the specified
local output path, or the current directory if no output path is
specified.


Experiments
-----------

Subcommands to manage experiments.


.. contents:: In this section:
  :local:
  :depth: 1

Create
~~~~~~

Create an experiment. The command has required argument for experiment name.
Additionally, you can provide an artifact location  using ``-l`` or ``--artifact-location``
option. If not provided, backend store will pick default location. Backend store will generate a
unique ID for each experiment.

All artifacts generated by runs related to this experiment will be stored under artifact location,
organized under specific run_uuid sub-directories.

Implementation of experiment and metadata store is dependent on backend storage. ``FileStore``
creates a folder for each experiment ID and stores metadata in ``meta.yaml``. Runs are stored as
subfolders.


List
~~~~

Lists all experiments managed by backend store. Command takes an optional ``--view`` or ``-v``
argument. Valid arguments are ``active_only`` (default), ``deleted_only``, or ``all``.


Delete
~~~~~~

Mark an active experiment for deletion. This also applies to experiment's metadata, runs and
associated data, and artifacts if they are store in default location. Use ``list`` command to view
artifact location. Command takes a required argument for experiment ID. Command will thrown
an error if experiment is not found or already marked for deletion.

Experiments marked for deletion can be restored using ``restore`` command, unless they are
permanently deleted.

Specific implementation of deletion is dependent on backend stores. ``FileStore`` moves
experiments marked for deletion under a ``.trash`` folder under the main folder used to
instantiate ``FileStore``. Experiments marked for deletion can be permanently deleted by clearing
the ``.trash`` folder. It is recommended to use a ``cron`` job or an alternate workflow mechanism
to clear ``.trash`` folder.


Restore
~~~~~~~

Restore a deleted experiment. This also applies to experiment's metadata, runs and associated data.
The command has a required argument for experiment ID. The command throws an error if the experiment is
already active, cannot be found, or permanently deleted.


Python Function Models
----------------------

Subcommands to serve Python models and apply them for inference.


Run
---

Run an MLflow project from the given URI.

If running locally (the default), the URI can be either a Git repository
URI or a local path. If running on Databricks, the URI must be a Git
repository.

By default, Git projects will run in a new working directory with the
given parameters, while local projects will run from the project's root
directory.


SageMaker Models
----------------

Subcommands to serve models on SageMaker.


scikit-learn Models
-------------------

Subcommands to serve scikit-learn models and apply them for inference.


UI
--

Run the MLflow tracking UI. The UI is served at http://localhost:5000.
